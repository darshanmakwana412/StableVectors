{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import ttools.modules\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random\n",
    "import pydiffvg\n",
    "import skimage\n",
    "import skimage.io\n",
    "import PIL\n",
    "import utils\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_paths = 500\n",
    "max_width = 2.0\n",
    "use_lpips_loss = False\n",
    "num_iter = 1000\n",
    "use_blob = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float32).to(device)\n",
    "\n",
    "height = 600\n",
    "width = 600\n",
    "num_images_per_prompt = 1\n",
    "num_inference_steps = num_iter\n",
    "guidance_scale = 7.5\n",
    "do_classifier_free_guidance = guidance_scale > 1.0\n",
    "generator = None\n",
    "\n",
    "pipeline.vae.requires_grad_(False)\n",
    "pipeline.unet.requires_grad_(False)\n",
    "pipeline.text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare prompt embeddings\n",
    "prompt = \"a panda rowing a boat minimal 2d vector graphics\"\n",
    "if prompt is not None and isinstance(prompt, str):\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = len(prompt)\n",
    "prompt_embeds = pipeline._encode_prompt(\n",
    "    prompt,\n",
    "    device,\n",
    "    num_images_per_prompt,\n",
    "    do_classifier_free_guidance,\n",
    ")\n",
    "\n",
    "# 4. Prepare timesteps\n",
    "pipeline.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "timesteps = pipeline.scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomPerspective(),\n",
    "    torchvision.transforms.RandomResizedCrop(size=(512, 512))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LPIPS is untested\n",
      " 62%|██████▏   | 617/1000 [05:27<03:23,  1.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14070/2818910619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         )[0]\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/diffusers/models/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[0m\n\u001b[1;32m    727\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                     \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m                 )\n\u001b[1;32m    731\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/diffusers/models/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m                 ).sample\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/diffusers/models/transformer_2d.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mclass_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             )\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/diffusers/models/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             )\n\u001b[1;32m    337\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/diffusers/models/attention_processor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_encoder_hidden_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pydiffvg.set_print_timing(False)\n",
    "\n",
    "gamma = 1.0\n",
    "\n",
    "# Set the device\n",
    "pydiffvg.set_use_gpu(True)\n",
    "\n",
    "perception_loss = ttools.modules.LPIPS().to(pydiffvg.get_device())\n",
    "\n",
    "# Load the image and scale it to [0, 1]\n",
    "# target = torch.from_numpy(skimage.io.imread(target)).to(torch.float32) / 255.0\n",
    "# target = target.pow(gamma)\n",
    "# target = target.to(pydiffvg.get_device())\n",
    "# target = target.unsqueeze(0)\n",
    "# target = target.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "#target = torch.nn.functional.interpolate(target, size = [256, 256], mode = 'area')\n",
    "canvas_width, canvas_height = width, height\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "shapes = []\n",
    "shape_groups = []\n",
    "if use_blob:\n",
    "    for i in range(num_paths):\n",
    "        num_segments = random.randint(3, 5)\n",
    "        num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
    "        points = []\n",
    "        p0 = (random.random(), random.random())\n",
    "        points.append(p0)\n",
    "        for j in range(num_segments):\n",
    "            radius = 0.05\n",
    "            p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
    "            p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
    "            p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
    "            points.append(p1)\n",
    "            points.append(p2)\n",
    "            if j < num_segments - 1:\n",
    "                points.append(p3)\n",
    "                p0 = p3\n",
    "        points = torch.tensor(points)\n",
    "        points[:, 0] *= canvas_width\n",
    "        points[:, 1] *= canvas_height\n",
    "        path = pydiffvg.Path(num_control_points = num_control_points,\n",
    "                                points = points,\n",
    "                                stroke_width = torch.tensor(1.0),\n",
    "                                is_closed = True)\n",
    "        shapes.append(path)\n",
    "        path_group = pydiffvg.ShapeGroup(shape_ids = torch.tensor([len(shapes) - 1]),\n",
    "                                            fill_color = torch.tensor([random.random(),\n",
    "                                                                    random.random(),\n",
    "                                                                    random.random(),\n",
    "                                                                    random.random()]))\n",
    "        shape_groups.append(path_group)\n",
    "else:\n",
    "    for i in range(num_paths):\n",
    "        num_segments = random.randint(1, 3)\n",
    "        num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
    "        points = []\n",
    "        p0 = (random.random(), random.random())\n",
    "        points.append(p0)\n",
    "        for j in range(num_segments):\n",
    "            radius = 0.05\n",
    "            p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
    "            p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
    "            p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
    "            points.append(p1)\n",
    "            points.append(p2)\n",
    "            points.append(p3)\n",
    "            p0 = p3\n",
    "        points = torch.tensor(points)\n",
    "        points[:, 0] *= canvas_width\n",
    "        points[:, 1] *= canvas_height\n",
    "        #points = torch.rand(3 * num_segments + 1, 2) * min(canvas_width, canvas_height)\n",
    "        path = pydiffvg.Path(num_control_points = num_control_points,\n",
    "                                points = points,\n",
    "                                stroke_width = torch.tensor(1.0),\n",
    "                                is_closed = False)\n",
    "        shapes.append(path)\n",
    "        path_group = pydiffvg.ShapeGroup(shape_ids = torch.tensor([len(shapes) - 1]),\n",
    "                                            fill_color = None,\n",
    "                                            stroke_color = torch.tensor([random.random(),\n",
    "                                                                        random.random(),\n",
    "                                                                        random.random(),\n",
    "                                                                        random.random()]))\n",
    "        shape_groups.append(path_group)\n",
    "\n",
    "scene_args = pydiffvg.RenderFunction.serialize_scene(canvas_width, canvas_height, shapes, shape_groups)\n",
    "\n",
    "render = pydiffvg.RenderFunction.apply\n",
    "img = render(canvas_width, # width\n",
    "                canvas_height, # height\n",
    "                2,   # num_samples_x\n",
    "                2,   # num_samples_y\n",
    "                0,   # seed\n",
    "                None,\n",
    "                *scene_args)\n",
    "pydiffvg.imwrite(img.cpu(), './init.png', gamma=gamma)\n",
    "\n",
    "points_vars = []\n",
    "stroke_width_vars = []\n",
    "color_vars = []\n",
    "for path in shapes:\n",
    "    path.points.requires_grad = True\n",
    "    points_vars.append(path.points)\n",
    "if not use_blob:\n",
    "    for path in shapes:\n",
    "        path.stroke_width.requires_grad = True\n",
    "        stroke_width_vars.append(path.stroke_width)\n",
    "if use_blob:\n",
    "    for group in shape_groups:\n",
    "        group.fill_color.requires_grad = True\n",
    "        color_vars.append(group.fill_color)\n",
    "else:\n",
    "    for group in shape_groups:\n",
    "        group.stroke_color.requires_grad = True\n",
    "        color_vars.append(group.stroke_color)\n",
    "\n",
    "# Optimizers for points, \n",
    "points_optim = torch.optim.Adam(points_vars, lr=1.0)\n",
    "if len(stroke_width_vars) > 0:\n",
    "    width_optim = torch.optim.Adam(stroke_width_vars, lr=0.1)\n",
    "color_optim = torch.optim.Adam(color_vars, lr=0.01)\n",
    "\n",
    "for t in tqdm(range(num_iter)):\n",
    "\n",
    "    points_optim.zero_grad()\n",
    "    if len(stroke_width_vars) > 0:\n",
    "        width_optim.zero_grad()\n",
    "    color_optim.zero_grad()\n",
    "    # Forward pass: render the image.\n",
    "    scene_args = pydiffvg.RenderFunction.serialize_scene(canvas_width, canvas_height, shapes, shape_groups)\n",
    "    img = render(canvas_width, \n",
    "                    canvas_height, \n",
    "                    2,   # num_samples_x\n",
    "                    2,   # num_samples_y\n",
    "                    t,   # seed\n",
    "                    None,\n",
    "                    *scene_args)\n",
    "\n",
    "    # Compose img with white background\n",
    "    img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
    "    # Save the intermediate render.\n",
    "    if t%100==0:\n",
    "        pydiffvg.imwrite(img.cpu(), 'results/iter_{}.png'.format(t), gamma=gamma)\n",
    "    img = img[:, :, :3]\n",
    "    # Convert img from HWC to NCHW\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "    \n",
    "    # 5. Prepare latents\n",
    "    img = transforms(img)\n",
    "    latents = pipeline.vae.encode(img.to(dtype=torch.float32)).latent_dist.sample()\n",
    "    latents = latents * pipeline.vae.config.scaling_factor\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn_like(latents)\n",
    "        latent_model_input = pipeline.scheduler.add_noise(latents, noise, torch.tensor(t))\n",
    "        latent_model_input = torch.cat([latent_model_input] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipeline.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # 6. Predict the noise residual\n",
    "        noise_pred = pipeline.unet(\n",
    "            latent_model_input,\n",
    "            t,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # 7. perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # 8. Compute the previous noisy sample x_t -> x_t-1\n",
    "        target = pipeline.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "        # target = pipeline.vae.decode(latents / pipeline.vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "    if use_lpips_loss:\n",
    "        loss = perception_loss(latents, target) + (latents.mean() - target.mean()).pow(2)\n",
    "    else:\n",
    "        loss = (latents - target).pow(2).mean()\n",
    "\n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    points_optim.step()\n",
    "    if len(stroke_width_vars) > 0:\n",
    "        width_optim.step()\n",
    "    color_optim.step()\n",
    "\n",
    "    if len(stroke_width_vars) > 0:\n",
    "        for path in shapes:\n",
    "            path.stroke_width.data.clamp_(1.0, max_width)\n",
    "    if use_blob:\n",
    "        for group in shape_groups:\n",
    "            group.fill_color.data.clamp_(0.0, 1.0)\n",
    "    else:\n",
    "        for group in shape_groups:\n",
    "            group.stroke_color.data.clamp_(0.0, 1.0)\n",
    "\n",
    "    # if t % 10 == 0 or t == num_iter - 1:\n",
    "    #     pydiffvg.save_svg('results/painterly_rendering/iter_{}.svg'.format(t), canvas_width, canvas_height, shapes, shape_groups)\n",
    "\n",
    "img = render(canvas_width, # width\n",
    "                canvas_height, # height\n",
    "                2,   # num_samples_x\n",
    "                2,   # num_samples_y\n",
    "                0,   # seed\n",
    "                None,\n",
    "                *scene_args)\n",
    "# Compose img with white background\n",
    "img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
    "# Save the intermediate render.\n",
    "pydiffvg.imwrite(img.cpu(), './final.png'.format(t), gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydiffvg.set_print_timing(False)\n",
    "\n",
    "gamma = 1.0\n",
    "\n",
    "# Set the device\n",
    "pydiffvg.set_use_gpu(True)\n",
    "\n",
    "perception_loss = ttools.modules.LPIPS().to(pydiffvg.get_device())\n",
    "\n",
    "# Load the image and scale it to [0, 1]\n",
    "target = torch.from_numpy(skimage.io.imread(target)).to(torch.float32) / 255.0\n",
    "target = target.pow(gamma)\n",
    "target = target.to(pydiffvg.get_device())\n",
    "target = target.unsqueeze(0)\n",
    "target = target.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "#target = torch.nn.functional.interpolate(target, size = [256, 256], mode = 'area')\n",
    "canvas_width, canvas_height = target.shape[3], target.shape[2]\n",
    "\n",
    "random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "shapes = []\n",
    "shape_groups = []\n",
    "if use_blob:\n",
    "    for i in range(num_paths):\n",
    "        num_segments = random.randint(3, 5)\n",
    "        num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
    "        points = []\n",
    "        p0 = (random.random(), random.random())\n",
    "        points.append(p0)\n",
    "        for j in range(num_segments):\n",
    "            radius = 0.05\n",
    "            p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
    "            p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
    "            p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
    "            points.append(p1)\n",
    "            points.append(p2)\n",
    "            if j < num_segments - 1:\n",
    "                points.append(p3)\n",
    "                p0 = p3\n",
    "        points = torch.tensor(points)\n",
    "        points[:, 0] *= canvas_width\n",
    "        points[:, 1] *= canvas_height\n",
    "        path = pydiffvg.Path(num_control_points = num_control_points,\n",
    "                                points = points,\n",
    "                                stroke_width = torch.tensor(1.0),\n",
    "                                is_closed = True)\n",
    "        shapes.append(path)\n",
    "        path_group = pydiffvg.ShapeGroup(shape_ids = torch.tensor([len(shapes) - 1]),\n",
    "                                            fill_color = torch.tensor([random.random(),\n",
    "                                                                    random.random(),\n",
    "                                                                    random.random(),\n",
    "                                                                    random.random()]))\n",
    "        shape_groups.append(path_group)\n",
    "else:\n",
    "    for i in range(num_paths):\n",
    "        num_segments = random.randint(1, 3)\n",
    "        num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
    "        points = []\n",
    "        p0 = (random.random(), random.random())\n",
    "        points.append(p0)\n",
    "        for j in range(num_segments):\n",
    "            radius = 0.05\n",
    "            p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
    "            p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
    "            p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
    "            points.append(p1)\n",
    "            points.append(p2)\n",
    "            points.append(p3)\n",
    "            p0 = p3\n",
    "        points = torch.tensor(points)\n",
    "        points[:, 0] *= canvas_width\n",
    "        points[:, 1] *= canvas_height\n",
    "        #points = torch.rand(3 * num_segments + 1, 2) * min(canvas_width, canvas_height)\n",
    "        path = pydiffvg.Path(num_control_points = num_control_points,\n",
    "                                points = points,\n",
    "                                stroke_width = torch.tensor(1.0),\n",
    "                                is_closed = False)\n",
    "        shapes.append(path)\n",
    "        path_group = pydiffvg.ShapeGroup(shape_ids = torch.tensor([len(shapes) - 1]),\n",
    "                                            fill_color = None,\n",
    "                                            stroke_color = torch.tensor([random.random(),\n",
    "                                                                        random.random(),\n",
    "                                                                        random.random(),\n",
    "                                                                        random.random()]))\n",
    "        shape_groups.append(path_group)\n",
    "\n",
    "scene_args = pydiffvg.RenderFunction.serialize_scene(canvas_width, canvas_height, shapes, shape_groups)\n",
    "\n",
    "render = pydiffvg.RenderFunction.apply\n",
    "img = render(canvas_width, # width\n",
    "                canvas_height, # height\n",
    "                2,   # num_samples_x\n",
    "                2,   # num_samples_y\n",
    "                0,   # seed\n",
    "                None,\n",
    "                *scene_args)\n",
    "pydiffvg.imwrite(img.cpu(), './init.png', gamma=gamma)\n",
    "\n",
    "points_vars = []\n",
    "stroke_width_vars = []\n",
    "color_vars = []\n",
    "for path in shapes:\n",
    "    path.points.requires_grad = True\n",
    "    points_vars.append(path.points)\n",
    "if not use_blob:\n",
    "    for path in shapes:\n",
    "        path.stroke_width.requires_grad = True\n",
    "        stroke_width_vars.append(path.stroke_width)\n",
    "if use_blob:\n",
    "    for group in shape_groups:\n",
    "        group.fill_color.requires_grad = True\n",
    "        color_vars.append(group.fill_color)\n",
    "else:\n",
    "    for group in shape_groups:\n",
    "        group.stroke_color.requires_grad = True\n",
    "        color_vars.append(group.stroke_color)\n",
    "\n",
    "# Optimizers for points, \n",
    "points_optim = torch.optim.Adam(points_vars, lr=1.0)\n",
    "if len(stroke_width_vars) > 0:\n",
    "    width_optim = torch.optim.Adam(stroke_width_vars, lr=0.1)\n",
    "color_optim = torch.optim.Adam(color_vars, lr=0.01)\n",
    "\n",
    "for t in tqdm(range(num_iter)):\n",
    "\n",
    "    points_optim.zero_grad()\n",
    "    if len(stroke_width_vars) > 0:\n",
    "        width_optim.zero_grad()\n",
    "    color_optim.zero_grad()\n",
    "    # Forward pass: render the image.\n",
    "    scene_args = pydiffvg.RenderFunction.serialize_scene(canvas_width, canvas_height, shapes, shape_groups)\n",
    "    img = render(canvas_width, \n",
    "                    canvas_height, \n",
    "                    2,   # num_samples_x\n",
    "                    2,   # num_samples_y\n",
    "                    t,   # seed\n",
    "                    None,\n",
    "                    *scene_args)\n",
    "\n",
    "    # Compose img with white background\n",
    "    img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
    "    # Save the intermediate render.\n",
    "\n",
    "    # pydiffvg.imwrite(img.cpu(), 'results/iter_{}.png'.format(t), gamma=gamma)\n",
    "    img = img[:, :, :3]\n",
    "    # Convert img from HWC to NCHW\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
    "\n",
    "    \n",
    "\n",
    "    if use_lpips_loss:\n",
    "        loss = perception_loss(img, target) + (img.mean() - target.mean()).pow(2)\n",
    "    else:\n",
    "        loss = (img - target).pow(2).mean()\n",
    "\n",
    "    # Backpropagate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    points_optim.step()\n",
    "    if len(stroke_width_vars) > 0:\n",
    "        width_optim.step()\n",
    "    color_optim.step()\n",
    "\n",
    "    if len(stroke_width_vars) > 0:\n",
    "        for path in shapes:\n",
    "            path.stroke_width.data.clamp_(1.0, max_width)\n",
    "    if use_blob:\n",
    "        for group in shape_groups:\n",
    "            group.fill_color.data.clamp_(0.0, 1.0)\n",
    "    else:\n",
    "        for group in shape_groups:\n",
    "            group.stroke_color.data.clamp_(0.0, 1.0)\n",
    "\n",
    "    # if t % 10 == 0 or t == num_iter - 1:\n",
    "    #     pydiffvg.save_svg('results/painterly_rendering/iter_{}.svg'.format(t), canvas_width, canvas_height, shapes, shape_groups)\n",
    "\n",
    "img = render(canvas_width, # width\n",
    "                canvas_height, # height\n",
    "                2,   # num_samples_x\n",
    "                2,   # num_samples_y\n",
    "                0,   # seed\n",
    "                None,\n",
    "                *scene_args)\n",
    "# Compose img with white background\n",
    "img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
    "# Save the intermediate render.\n",
    "pydiffvg.imwrite(img.cpu(), './final.png'.format(t), gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 7. Denoising loop\n",
    "num_warmup_steps = len(timesteps) - num_inference_steps * pipeline.scheduler.order\n",
    "with pipeline.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "    for i, t in enumerate(timesteps):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipeline.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        noise_pred = pipeline.unet(\n",
    "            latent_model_input,\n",
    "            t,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        # perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = pipeline.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "        # call the callback, if provided\n",
    "        if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipeline.scheduler.order == 0):\n",
    "            progress_bar.update()\n",
    "\n",
    "image = pipeline.vae.decode(latents / pipeline.vae.config.scaling_factor, return_dict=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = utils.denormalize(image)\n",
    "image = utils.pt_to_numpy(image)\n",
    "image = utils.numpy_to_pil(image)\n",
    "image[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
